{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATS VS DOGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranged the files into directories. 1 directory per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#files = os.listdir(\"data/images\")\n",
    "#for filename in files:\n",
    "#    classname = filename.rsplit(\"_\", 1)[0]\n",
    "#    source = os.path.join(\"data/images\", filename)\n",
    "#    dest_dir = os.path.join(\"data/images\", classname)\n",
    "#    destination = os.path.join(dest_dir, filename)\n",
    "#    print(source, destination)\n",
    "#    os.makedirs(dest_dir, exist_ok=True)\n",
    "#    os.rename(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = VGG19(\n",
    "    weights=\"imagenet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 143,667,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make and add our new layer in place of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer = Dense(37, activation='softmax', name='Testing')\n",
    "\n",
    "inp = baseModel.input\n",
    "out = new_layer(baseModel.layers[-2].output)\n",
    "\n",
    "model = Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevent all layers from retraining except the last 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers \n",
    "for layer in model.layers[:-3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "Testing (Dense)              (None, 37)                151589    \n",
      "=================================================================\n",
      "Total params: 139,721,829\n",
      "Trainable params: 119,697,445\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Construction\n",
    "In this next section, we created the objects that will seperate the test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/images\"\n",
    "targetSize = (224,224)\n",
    "batchSize = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5913 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=targetSize,\n",
    "    batch_size=batchSize,\n",
    "    class_mode='binary',\n",
    "    subset='training') # set as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1477 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    path, # same directory as training data\n",
    "    target_size=targetSize,\n",
    "    batch_size=batchSize,\n",
    "    class_mode='binary',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time!\n",
    "Here we trained the model and evaluated it after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=SGD(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-4da7326373ea>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "184/184 [==============================] - 146s 792ms/step - loss: 1.2697 - accuracy: 0.6383 - val_loss: 0.8897 - val_accuracy: 0.7303\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 67s 366ms/step - loss: 0.1681 - accuracy: 0.9464 - val_loss: 0.7551 - val_accuracy: 0.7806\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 58s 315ms/step - loss: 0.0168 - accuracy: 0.9973 - val_loss: 0.7347 - val_accuracy: 0.8139\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 58s 315ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.8145\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 58s 317ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7466 - val_accuracy: 0.8152\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 58s 317ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7570 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 58s 318ms/step - loss: 9.4270e-04 - accuracy: 1.0000 - val_loss: 0.7666 - val_accuracy: 0.8125\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 59s 319ms/step - loss: 7.9930e-04 - accuracy: 1.0000 - val_loss: 0.7667 - val_accuracy: 0.8152\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 59s 319ms/step - loss: 6.9738e-04 - accuracy: 1.0000 - val_loss: 0.7816 - val_accuracy: 0.8132\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 59s 319ms/step - loss: 6.1738e-04 - accuracy: 1.0000 - val_loss: 0.7861 - val_accuracy: 0.8152\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batchSize,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batchSize,\n",
    "    epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model\n",
    "We save the model here so that we do not have to re-train the model every time that we want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eric\\anaconda3\\envs\\mnist\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Eric\\anaconda3\\envs\\mnist\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: Model Saves/CvD V1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Model Saves/CvD V1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/\n",
    "* https://keras.io/api/preprocessing/image/#flowfromdirectory-method\n",
    "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "* https://keras.io/api/applications/vgg/\n",
    "* https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "* https://keras.io/api/preprocessing/#image-data-preprocessing\n",
    "* https://stackoverflow.com/questions/41378461/how-to-use-models-from-keras-applications-for-transfer-learnig/41386444#41386444\n",
    "* https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "Here we load the model that we have saved in previous executions of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model saves/CvD V1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutaion Time\n",
    "Although we evaluated using the model.fit_generator function above, we wanted to get a another look to confirm our accuracy. We did this by printing the output of model.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 12s 249ms/step - loss: 0.7839 - accuracy: 0.0325\n",
      "> 3.250\n"
     ]
    }
   ],
   "source": [
    "loss , acc = model.evaluate(validation_generator, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map Creation:\n",
    "This next section allows us to create a heatmap that will show which animals were misclassified the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1477 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(\n",
    "    path, # same directory as training data\n",
    "    target_size=targetSize,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    class_mode='binary',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Batch:  79\n",
      "Batch:  80\n",
      "Batch:  81\n",
      "Batch:  82\n",
      "Batch:  83\n",
      "Batch:  84\n",
      "Batch:  85\n",
      "Batch:  86\n",
      "Batch:  87\n",
      "Batch:  88\n",
      "Batch:  89\n",
      "Batch:  90\n",
      "Batch:  91\n",
      "Batch:  92\n",
      "Batch:  93\n",
      "Batch:  94\n",
      "Batch:  95\n",
      "Batch:  96\n",
      "Batch:  97\n",
      "Batch:  98\n",
      "Batch:  99\n",
      "Batch:  100\n",
      "Batch:  101\n",
      "Batch:  102\n",
      "Batch:  103\n",
      "Batch:  104\n",
      "Batch:  105\n",
      "Batch:  106\n",
      "Batch:  107\n",
      "Batch:  108\n",
      "Batch:  109\n",
      "Batch:  110\n",
      "Batch:  111\n",
      "Batch:  112\n",
      "Batch:  113\n",
      "Batch:  114\n",
      "Batch:  115\n",
      "Batch:  116\n",
      "Batch:  117\n",
      "Batch:  118\n",
      "Batch:  119\n",
      "Batch:  120\n",
      "Batch:  121\n",
      "Batch:  122\n",
      "Batch:  123\n",
      "Batch:  124\n",
      "Batch:  125\n",
      "Batch:  126\n",
      "Batch:  127\n",
      "Batch:  128\n",
      "Batch:  129\n",
      "Batch:  130\n",
      "Batch:  131\n",
      "Batch:  132\n",
      "Batch:  133\n",
      "Batch:  134\n",
      "Batch:  135\n",
      "Batch:  136\n",
      "Batch:  137\n",
      "Batch:  138\n",
      "Batch:  139\n",
      "Batch:  140\n",
      "Batch:  141\n",
      "Batch:  142\n",
      "Batch:  143\n",
      "Batch:  144\n",
      "Batch:  145\n",
      "Batch:  146\n",
      "Batch:  147\n",
      "Batch:  148\n",
      "Batch:  149\n",
      "Batch:  150\n",
      "Batch:  151\n",
      "Batch:  152\n",
      "Batch:  153\n",
      "Batch:  154\n",
      "Batch:  155\n",
      "Batch:  156\n",
      "Batch:  157\n",
      "Batch:  158\n",
      "Batch:  159\n",
      "Batch:  160\n",
      "Batch:  161\n",
      "Batch:  162\n",
      "Batch:  163\n",
      "Batch:  164\n",
      "Batch:  165\n",
      "Batch:  166\n",
      "Batch:  167\n",
      "Batch:  168\n",
      "Batch:  169\n",
      "Batch:  170\n",
      "Batch:  171\n",
      "Batch:  172\n",
      "Batch:  173\n",
      "Batch:  174\n",
      "Batch:  175\n",
      "Batch:  176\n",
      "Batch:  177\n",
      "Batch:  178\n",
      "Batch:  179\n",
      "Batch:  180\n",
      "Batch:  181\n",
      "Batch:  182\n",
      "Batch:  183\n",
      "Batch:  184\n",
      "Batch:  185\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 37\n",
    "wrong = np.zeros((size,size))\n",
    "wrongPics = np.zeros((size,size),dtype = np.int32)\n",
    "\n",
    "for batchNum,(xs,ys) in enumerate(test_generator):\n",
    "    yPredict = model.predict(xs)\n",
    "    categories = np.argmax(yPredict,axis = 1)\n",
    "    \n",
    "    for i, (predicted, actual) in enumerate(zip(categories,ys.astype(np.int32))):\n",
    "        if predicted != actual:\n",
    "            wrong[predicted,actual] += 1\n",
    "            wrongPics[predicted,actual] = i\n",
    "    print(\"Batch: \",batchNum)\n",
    "    if(batchNum > test_generator.samples // 8):\n",
    "        break\n",
    "pyplot.imshow(wrong, cmap=pyplot.get_cmap('Reds'))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that Pitbull Terriers and boxers were most mistaken for each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the index of the animals that were most mistaken and total incorrect predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "8\n",
      "9.0\n",
      "274.0\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(wrong)//37)\n",
    "print(np.argmax(wrong)%37)\n",
    "print(np.max(wrong))\n",
    "print(np.sum(wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
